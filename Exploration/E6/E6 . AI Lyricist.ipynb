{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6. 작사가 인공지능 만들기\n",
    "\n",
    "`21 JAN 2021`\n",
    "\n",
    "----------\n",
    "\n",
    ">1. 데이터 다운로드\n",
    ">2. 데이터 읽어오기\n",
    ">3. 데이터 정제\n",
    ">4. 평가 데이터셋 분리\n",
    ">5. 인공지능 만들기\n",
    ">6. 작곡해보기\n",
    "\n",
    "## 6-7. 프로젝트: 멋진 작사가 만들기\n",
    "\n",
    "### Step 1. 데이터 다운로드\n",
    "\n",
    "먼저 아래 링크에서 Song Lyrics 데이터를 다운로드해 주세요! 저장된 파일을 압축 해제한 후, 모든 txt 파일을 lyrics 폴더를 만들어 그 속에 저장해주세요!\n",
    "\n",
    "Song Lyrics\n",
    "\n",
    "아래의 명령어를 실행하셔도 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip   \n",
    "    \n",
    "$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 import하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기\n",
    "\n",
    "glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['I. LIFE.', '', '']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제\n",
    "\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠.   \n",
    "\n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "187088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I. LIFE.', '', '', '', '', '', 'I.', '', 'SUCCESS.', '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(raw_corpus))\n",
    "print(len(raw_corpus))\n",
    "raw_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)      \n",
    "    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)               \n",
    "    # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'     \n",
    "    # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\")) \n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i . life . <end>',\n",
       " '<start> i . <end>',\n",
       " '<start> success . <end>',\n",
       " '<start> published in a masque of poets <end>',\n",
       " '<start> at the request of h . h . , the author s <end>',\n",
       " '<start> fellow townswoman and friend . <end>',\n",
       " '<start> success is counted sweetest <end>',\n",
       " '<start> by those who ne er succeed . <end>',\n",
       " '<start> to comprehend a nectar <end>',\n",
       " '<start> requires sorest need . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 13: continue  # 여기서 헷갈렸다.\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화\n",
    "\n",
    "토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하는법은~!?\n",
    "\n",
    ": 토큰화 과정에 maxlen = 15를 추가해줘야한다.\n",
    "\n",
    "- tokenize함수에서 maxlen을 설정하면 val_loss값을 낮추기 어렵다는데 왜그럴까?   \n",
    ": 강제로 길이를 지정해서 잘라서 그런걸까?\n",
    "\n",
    "처음에 이과정 없이 해버렸었다. \n",
    "\n",
    "\n",
    "`tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=None, dtype='int32', padding='pre'or'post',\n",
    "    truncating='pre', value=0.0\n",
    ")`\n",
    "\n",
    "- truncating\n",
    "   \n",
    "Tensorflow keras의 pad_sequences의 truncating 파라미터는 maxlen보다 더 긴 문장이 들어왔을 때 해당 문장을 maxlen에 맞춰서 자를 때와 관련이 있다.\n",
    "   \n",
    "pre-padding과 post-padding에 마찬가지로 truncating='pre'이면 앞에서부터 잘라나가고, truncating='post'이면 뒤에서부터 단어가 잘려나간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    5   19 ...    0    0    0]\n",
      " [   2    5   19 ...    0    0    0]\n",
      " [   2 2458   19 ...    0    0    0]\n",
      " ...\n",
      " [   2   77   47 ...    3    0    0]\n",
      " [   2   49    4 ...    0    0    0]\n",
      " [   2   13  631 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f9b3fb77a50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   5  19 107  19   3   0   0   0   0   0   0   0   0]\n",
      "[  5  19 107  19   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 셋 구축 하기 \n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1 # 0:<pad>를 포함하여 dictionary 갯수 + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리\n",
    "\n",
    "훈련 데이터와 평가 데이터를 분리하세요!\n",
    "\n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!\n",
    "\n",
    "`train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (126924, 14)\n",
      "Target Train: (126924, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 할때 maxlen = 15로 조절을 했는데 왜 노드에서의 가이드라인의 \n",
    "\n",
    "out:\n",
    "\n",
    "Source Train: (124960, 14)\n",
    "Target Train: (124960, 14)\n",
    "\n",
    "학습데이터 개수가 124960이 아닌, 126924가 나올까?\n",
    "\n",
    "- 1차 maxlen = 15 로 조절 \n",
    "\n",
    "Source Train: (126924, 14)\n",
    "Target Train: (126924, 14)\n",
    "\n",
    "어떻게 해야 조절 할 수 있을까? 데이터 정제과정에서 다시 찾아보자!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기\n",
    "\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.14013659e-04,  1.87222540e-05, -9.66627122e-05, ...,\n",
       "         -2.60603934e-04, -6.48406276e-05, -8.26021715e-05],\n",
       "        [-2.89371528e-04,  1.97988062e-04,  5.58906941e-05, ...,\n",
       "         -4.19346616e-04, -1.82846299e-04, -2.39896981e-04],\n",
       "        [-3.40478378e-04, -1.46817431e-04, -2.39466186e-04, ...,\n",
       "         -3.35652498e-04, -3.52894043e-04, -2.04910451e-04],\n",
       "        ...,\n",
       "        [ 8.19761772e-04, -6.65547792e-04, -2.97705905e-04, ...,\n",
       "         -1.45620463e-04,  8.11505131e-04, -9.61733866e-04],\n",
       "        [ 1.33755920e-03, -7.48955121e-04,  3.37372549e-05, ...,\n",
       "          2.04635653e-04,  7.77511683e-04, -7.45207246e-04],\n",
       "        [ 1.78137515e-03, -9.05660854e-04,  3.94614646e-04, ...,\n",
       "          6.48431713e-04,  6.38856378e-04, -4.80403542e-04]],\n",
       "\n",
       "       [[-1.14013659e-04,  1.87222540e-05, -9.66627122e-05, ...,\n",
       "         -2.60603934e-04, -6.48406276e-05, -8.26021715e-05],\n",
       "        [-2.02447394e-04, -1.81426716e-04, -6.67339045e-05, ...,\n",
       "         -3.33542790e-04, -1.92076535e-04,  9.83178033e-05],\n",
       "        [-8.54990285e-05, -4.52083797e-04, -3.75480391e-04, ...,\n",
       "         -2.74105900e-04, -3.33092059e-04,  1.25549283e-04],\n",
       "        ...,\n",
       "        [ 7.70031998e-04, -7.10113731e-04, -5.88221184e-04, ...,\n",
       "          1.10807386e-03, -7.86879042e-04,  4.10076289e-04],\n",
       "        [ 1.23248692e-03, -9.22860869e-04, -3.78121505e-04, ...,\n",
       "          1.48925907e-03, -7.72733300e-04,  4.41501965e-04],\n",
       "        [ 1.63804879e-03, -1.17358356e-03, -1.52662717e-04, ...,\n",
       "          1.88505638e-03, -8.17024033e-04,  5.03897085e-04]],\n",
       "\n",
       "       [[-1.14013659e-04,  1.87222540e-05, -9.66627122e-05, ...,\n",
       "         -2.60603934e-04, -6.48406276e-05, -8.26021715e-05],\n",
       "        [-2.58078479e-04,  3.48320282e-05, -6.53236639e-05, ...,\n",
       "         -4.59303788e-04,  2.84996731e-05, -2.91643722e-04],\n",
       "        [-3.44037835e-04,  2.04299999e-04,  9.51084949e-05, ...,\n",
       "         -7.31918612e-04, -7.59062677e-05, -4.27841005e-04],\n",
       "        ...,\n",
       "        [ 1.14541710e-03, -6.35819975e-04,  1.36739446e-03, ...,\n",
       "          1.20811572e-03, -5.15448619e-06,  3.26951849e-04],\n",
       "        [ 1.53375021e-03, -8.69833981e-04,  1.57264341e-03, ...,\n",
       "          1.65380910e-03, -1.07418869e-04,  4.88804770e-04],\n",
       "        [ 1.85465126e-03, -1.12986739e-03,  1.73664954e-03, ...,\n",
       "          2.05939263e-03, -2.72194680e-04,  6.52371964e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.14013659e-04,  1.87222540e-05, -9.66627122e-05, ...,\n",
       "         -2.60603934e-04, -6.48406276e-05, -8.26021715e-05],\n",
       "        [-4.73848806e-04, -1.15694587e-04, -1.88117090e-04, ...,\n",
       "         -4.86910983e-04, -2.16491550e-04, -1.16601577e-05],\n",
       "        [-8.93197372e-04,  6.93622269e-06, -6.25824032e-04, ...,\n",
       "         -5.10016922e-04, -4.54712776e-04,  6.46624030e-05],\n",
       "        ...,\n",
       "        [ 9.11106239e-04, -1.75439738e-04,  2.27961340e-04, ...,\n",
       "         -5.18135494e-04,  2.35700616e-04,  4.66426514e-04],\n",
       "        [ 1.36292155e-03, -4.56000678e-04,  5.14036044e-04, ...,\n",
       "          2.25897065e-06,  2.28812714e-04,  5.93741133e-04],\n",
       "        [ 1.74065365e-03, -7.58808281e-04,  7.77769659e-04, ...,\n",
       "          5.59462176e-04,  1.26879429e-04,  7.38681701e-04]],\n",
       "\n",
       "       [[-1.14013659e-04,  1.87222540e-05, -9.66627122e-05, ...,\n",
       "         -2.60603934e-04, -6.48406276e-05, -8.26021715e-05],\n",
       "        [-1.55594607e-04,  2.02824322e-05, -4.24754035e-05, ...,\n",
       "         -3.73063289e-04, -1.17987533e-04, -2.41713686e-04],\n",
       "        [-3.64127773e-04,  1.52426088e-04, -1.19334676e-04, ...,\n",
       "         -6.71126996e-04, -3.87110224e-04, -4.68688639e-04],\n",
       "        ...,\n",
       "        [ 1.75998954e-03, -2.09071659e-04,  1.05086714e-03, ...,\n",
       "         -4.33550129e-04, -2.89529089e-05, -6.05524974e-06],\n",
       "        [ 2.12468044e-03, -4.09783504e-04,  1.32314442e-03, ...,\n",
       "          3.99001146e-05, -1.01631289e-04,  1.30124710e-04],\n",
       "        [ 2.42127734e-03, -6.61859463e-04,  1.53821812e-03, ...,\n",
       "          5.51098434e-04, -2.43374569e-04,  2.97708088e-04]],\n",
       "\n",
       "       [[-1.14013659e-04,  1.87222540e-05, -9.66627122e-05, ...,\n",
       "         -2.60603934e-04, -6.48406276e-05, -8.26021715e-05],\n",
       "        [-1.85903322e-04, -2.46883399e-04, -2.52280995e-04, ...,\n",
       "         -4.63239820e-04,  1.51679400e-04, -1.15680989e-04],\n",
       "        [-3.88951012e-04, -5.64939808e-04, -5.24163130e-04, ...,\n",
       "         -3.76045617e-04,  2.32811617e-05,  9.73582428e-06],\n",
       "        ...,\n",
       "        [ 6.71526534e-04, -4.16899769e-04,  1.15892908e-04, ...,\n",
       "         -9.89779015e-04,  8.49587494e-04,  5.84962894e-04],\n",
       "        [ 1.09894830e-03, -5.27274155e-04,  4.35324968e-04, ...,\n",
       "         -4.86482517e-04,  7.99975824e-04,  6.64505875e-04],\n",
       "        [ 1.50197628e-03, -6.95496215e-04,  7.23980309e-04, ...,\n",
       "          1.12147362e-04,  6.49083522e-04,  7.69915408e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델의 데이터를 확인해보자\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "619/619 [==============================] - 82s 133ms/step - loss: 3.4601\n",
      "Epoch 2/10\n",
      "619/619 [==============================] - 83s 134ms/step - loss: 3.0025\n",
      "Epoch 3/10\n",
      "619/619 [==============================] - 84s 135ms/step - loss: 2.8273\n",
      "Epoch 4/10\n",
      "619/619 [==============================] - 83s 135ms/step - loss: 2.6911\n",
      "Epoch 5/10\n",
      "619/619 [==============================] - 84s 135ms/step - loss: 2.5751\n",
      "Epoch 6/10\n",
      "619/619 [==============================] - 84s 135ms/step - loss: 2.4709\n",
      "Epoch 7/10\n",
      "619/619 [==============================] - 84s 135ms/step - loss: 2.3736\n",
      "Epoch 8/10\n",
      "619/619 [==============================] - 84s 135ms/step - loss: 2.2831\n",
      "Epoch 9/10\n",
      "619/619 [==============================] - 81s 130ms/step - loss: 2.1975\n",
      "Epoch 10/10\n",
      "619/619 [==============================] - 79s 128ms/step - loss: 2.1161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b3fb77550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. 모델이 잘 만들어 졌는지 평가하기\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                    tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 실제로 위 문장 생성 함수를 실행해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> like a slumber party <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> like \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> say it all , <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> say \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> fall asleep <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> fall \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> do you remember the time <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> Do you \", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 정리\n",
    "\n",
    "if len(sentence.split()) >= 13: continue\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "- `loss: 2.0713`\n",
    "\n",
    "\n",
    "i love you , i m not gonna crack    \n",
    "like a carrot baby      \n",
    "say it all     \n",
    "fall in love       \n",
    "do you mind ? do you mind ? do you mind ?      \n",
    "    \n",
    "    \n",
    "- 새로운 모델을 진행했는데 노트북에 문제가 생겨서 삭제하고 다시 진행했더니 \n",
    "\n",
    "- `loss: 2.1161` \n",
    "  \n",
    "i love you , i love you \n",
    "like a slumber party \n",
    "say it all ,\n",
    "fall asleep\n",
    "do you remember the time \n",
    "    \n",
    "문장도 다르게 나왔다... 이상하다...?\n",
    "\n",
    "    \n",
    "- 바꿀 수 있는 조건들을 변경해서 여러번 시도해보았다. \n",
    "\n",
    "바꾼 조건\n",
    "       \n",
    "if len(sentence.split()) >=15 : continue   \n",
    "   \n",
    "embedding_size = 512   \n",
    "hidden_size = 2084    \n",
    "\n",
    "- `loss: 0.9899`    \n",
    "    \n",
    " \n",
    "i love you so , hey   \n",
    "like a complete unknown \n",
    "say you need me , say you need me   \n",
    "fall asleep all alone  \n",
    "do you remember\n",
    "    \n",
    "- 내가 하고싶은대로 조건을 바꿔서 기준이 애매하다. 기준을 정해서 다시 시도해봐야겠다고 생각했다.\n",
    "두번째 시도에서는 loss는 줄어들었으나 내 개인적으로 만들어진 문장이 더 어색하다고 느껴졌다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * embedding_size 를 512로 변경\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model1 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "619/619 [==============================] - 87s 141ms/step - loss: 3.3889\n",
      "Epoch 2/10\n",
      "619/619 [==============================] - 88s 142ms/step - loss: 2.9409\n",
      "Epoch 3/10\n",
      "619/619 [==============================] - 93s 150ms/step - loss: 2.7557\n",
      "Epoch 4/10\n",
      "619/619 [==============================] - 94s 152ms/step - loss: 2.6141\n",
      "Epoch 5/10\n",
      "619/619 [==============================] - 96s 155ms/step - loss: 2.4905\n",
      "Epoch 6/10\n",
      "619/619 [==============================] - 95s 154ms/step - loss: 2.3779\n",
      "Epoch 7/10\n",
      "619/619 [==============================] - 95s 153ms/step - loss: 2.2733\n",
      "Epoch 8/10\n",
      "619/619 [==============================] - 94s 152ms/step - loss: 2.1743\n",
      "Epoch 9/10\n",
      "619/619 [==============================] - 93s 150ms/step - loss: 2.0814\n",
      "Epoch 10/10\n",
      "619/619 [==============================] - 94s 152ms/step - loss: 1.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b3f5b03d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "model1.compile(loss=loss, optimizer=optimizer)\n",
    "model1.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ -1.9931582 ,   2.1257358 , -10.937791  , ..., -14.723316  ,\n",
       "         -17.166977  , -10.882372  ],\n",
       "        [ -5.667493  ,  -1.8443996 , -15.25288   , ..., -20.664396  ,\n",
       "         -23.469477  , -15.582147  ],\n",
       "        [ -8.155229  ,   1.2001386 , -16.682602  , ..., -25.432053  ,\n",
       "         -27.889135  , -16.761204  ],\n",
       "        ...,\n",
       "        [ -6.953226  ,  -0.9529557 , -17.084349  , ..., -27.544895  ,\n",
       "         -24.850405  , -17.319046  ],\n",
       "        [ -8.310036  ,  -2.9983947 , -19.531599  , ..., -21.685534  ,\n",
       "         -31.825495  , -19.581793  ],\n",
       "        [ -6.5922003 ,  -1.4776088 , -17.617447  , ..., -27.897814  ,\n",
       "         -25.057594  , -17.683277  ]],\n",
       "\n",
       "       [[ -5.0596466 ,   3.157425  , -11.387841  , ..., -10.658058  ,\n",
       "         -10.520708  , -11.446289  ],\n",
       "        [ -8.786496  ,   0.71129084, -20.850794  , ..., -26.294525  ,\n",
       "         -35.642498  , -20.935694  ],\n",
       "        [ -8.547397  ,   0.85710967, -19.782425  , ..., -22.91139   ,\n",
       "         -31.327     , -19.966875  ],\n",
       "        ...,\n",
       "        [  6.6519747 , -10.191485  , -18.224478  , ..., -39.57424   ,\n",
       "         -25.646805  , -18.088463  ],\n",
       "        [  6.720071  , -10.285258  , -18.328945  , ..., -39.549595  ,\n",
       "         -26.256128  , -18.18112   ],\n",
       "        [  6.7736735 , -10.333235  , -18.374475  , ..., -39.441936  ,\n",
       "         -26.575577  , -18.219261  ]],\n",
       "\n",
       "       [[ -5.0596466 ,   3.157425  , -11.387841  , ..., -10.658058  ,\n",
       "         -10.520708  , -11.446289  ],\n",
       "        [ -7.9282713 ,   2.4224195 , -17.294333  , ..., -22.669846  ,\n",
       "         -20.777813  , -17.35608   ],\n",
       "        [ -9.273911  ,   1.422911  , -18.595493  , ..., -22.883852  ,\n",
       "         -30.458418  , -18.732264  ],\n",
       "        ...,\n",
       "        [  6.822849  ,  -9.813812  , -17.615976  , ..., -36.81772   ,\n",
       "         -24.39822   , -17.607693  ],\n",
       "        [  6.836525  ,  -9.853261  , -18.138382  , ..., -38.617397  ,\n",
       "         -25.960402  , -18.09983   ],\n",
       "        [  6.791994  ,  -9.950085  , -18.19296   , ..., -38.775333  ,\n",
       "         -26.560284  , -18.127329  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -5.595962  ,  -0.50270224, -17.612171  , ..., -24.668957  ,\n",
       "         -26.806261  , -17.729462  ],\n",
       "        [ -7.661839  ,  -0.93920684, -17.32597   , ..., -23.366003  ,\n",
       "         -27.643532  , -17.472439  ],\n",
       "        [ -9.851524  ,   0.33655274, -19.142864  , ..., -17.629263  ,\n",
       "         -26.717997  , -19.283278  ],\n",
       "        ...,\n",
       "        [ -6.7183204 ,   2.4884653 , -16.262056  , ..., -15.514005  ,\n",
       "         -25.815325  , -16.510197  ],\n",
       "        [ -5.9119616 ,  -2.47347   , -18.17604   , ..., -26.763105  ,\n",
       "         -30.896002  , -18.442535  ],\n",
       "        [ -6.194885  ,  -0.7567643 , -17.172087  , ..., -26.586596  ,\n",
       "         -22.518076  , -17.417759  ]],\n",
       "\n",
       "       [[ -5.0596466 ,   3.157425  , -11.387841  , ..., -10.658058  ,\n",
       "         -10.520708  , -11.446289  ],\n",
       "        [ -7.5487413 ,   1.9718155 , -17.833244  , ..., -22.236685  ,\n",
       "         -29.551367  , -17.58332   ],\n",
       "        [ -5.7666245 ,   3.1716332 , -14.177691  , ..., -13.564202  ,\n",
       "         -21.538883  , -14.23589   ],\n",
       "        ...,\n",
       "        [  6.0729375 , -10.055451  , -17.781872  , ..., -37.25335   ,\n",
       "         -26.181904  , -17.827396  ],\n",
       "        [  6.0916896 , -10.058336  , -17.77926   , ..., -37.234753  ,\n",
       "         -26.22391   , -17.818134  ],\n",
       "        [  6.118863  , -10.080643  , -17.773968  , ..., -37.212708  ,\n",
       "         -26.269941  , -17.805393  ]],\n",
       "\n",
       "       [[ -5.0596466 ,   3.157425  , -11.387841  , ..., -10.658058  ,\n",
       "         -10.520708  , -11.446289  ],\n",
       "        [ -8.884569  ,   2.0357525 , -20.230978  , ..., -25.10743   ,\n",
       "         -27.86745   , -20.300001  ],\n",
       "        [ -4.023554  ,   2.9902637 , -15.118299  , ..., -15.803468  ,\n",
       "         -20.019493  , -15.082592  ],\n",
       "        ...,\n",
       "        [  6.8925347 , -10.395425  , -18.27438   , ..., -38.256313  ,\n",
       "         -26.359495  , -18.16005   ],\n",
       "        [  6.7504687 , -10.346383  , -18.29499   , ..., -38.362366  ,\n",
       "         -26.864685  , -18.153982  ],\n",
       "        [  6.7317653 , -10.390732  , -18.292639  , ..., -38.362865  ,\n",
       "         -26.97679   , -18.144669  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델의 데이터를 확인해보자\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model1(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you lie <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model1, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> like a slumber party <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model1, tokenizer, init_sentence=\"<start> like \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> say it ain t no use <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model1, tokenizer, init_sentence=\"<start> say \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> fall asleep <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> fall \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> do you remember the time <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> Do you \", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1 결과 정리\n",
    "\n",
    "- embedding_size = 512 hidden_size = 1024\n",
    "\n",
    "\n",
    "- loss: 1.9945\n",
    "\n",
    "작사결과\n",
    "\n",
    "i love the way you lie \n",
    "like a slumber party  \n",
    "say it ain t no use\n",
    "fall asleep    \n",
    "do you remember the time \n",
    "    \n",
    "오 뭔가 좀더 자연스러워 진것 같았다!! loss도 1점 후반대의 결과를 얻었다."
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어려움\n",
    "\n",
    "- 훈련데이터와 평가데이터를 나누는데 (124960,14) 이렇게 나와야 하는데 전처리 후에 학습데이터 갯수가 124960보다 계속 크게 나와서 이 과정에서 시간이 오래걸렸다. 팀원 분께서 조언 해주셔서 시도해보았는데 원인을 잘 모르겠다. \n",
    "   \n",
    "- epoch를 20이상 하면 loss가 1점 후반대로 줄어든다. 다만 10으로 조절 했을 때 최소가 loss: 2.0713 였다.\n",
    "  다른 팀원 분은 epoch 10으로도 loss 1점대의 결과를 얻었다고 하셔서 어떤것을 조절해야 더 좋은 성능을 가질까 고민했다. \n",
    "\n",
    "\n",
    "## 느낀점 \n",
    "\n",
    "- 처음에 batch_size를 512로 했었는데 256으로 바꿔서 진행해보았다. 시간의 차이는 얼마 없었다. \n",
    "  batch_size는 크게 영향을 주진 않나보다라고 생각했다. \n",
    "\n",
    "- 팀원분의 pre padding vs post padding 의 질문에 궁금해서 이리저리 찾아보았다. 저번에 노드에서와 마찬가지로 이론 상으로는 pre padding의 성능이 더 좋지만, 실제로는 post padding이 더 좋아서 많이 쓴다라는 퍼실님의 답변이 있었다. 그리고 찾아보니 모델별로 결과가 다르다는 것도 알게 되었다. \n",
    "\n",
    "   \n",
    "논문의 결과에 따르면 LSTM의 경우  pre-padding 전처리를 수행했을 때가 post-padding 전처리를 수행했을 때보다 성능이 뛰어났다고함.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "또, CNN의 경우 pre-padding이나 post-padding의 성능이 유사했는데, 이는 시퀀스 모델링과는 상관없는 CNN의 특성을 반영하였다고 한다. \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "논문의 신뢰성이 조금은 떨어진다는데 다른 논문들도 찾아봐야 겠다는 생각을 했다.\n",
    "\n",
    "- 당연히 작사를 더 자연스럽게 하려면 단어의 수를 높이면 가능할거라고 생각한다. 다만 '자연스럽다' 라는 의미는 많은 단어를 학습시키는 것도 중요하지만, 매끄러운 문장, 더 수준있는 문장을 만들려면 어떤 부분이 더 중요할까?\n",
    "\n",
    "\n",
    "- 영어로 작사를 하게하는 노드를 진행하니 한국어로도 진행해보고 싶다는 생각이 들었다. 찾아보면서 언어의 특성이 중요하다 라는 것을 생각하게 되었는데 한국어는 토큰화가 어렵다,, 또 이외에도 많은 어려움이 있는것을 보았지만 도전은 해보고싶다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부족한 개념 알고가자!\n",
    "\n",
    "- 토큰화 : https://wikidocs.net/21698\n",
    "\n",
    "- 패딩 : https://wikidocs.net/83544\n",
    "\n",
    "- 순환 신경망(RNN): https://wikidocs.net/22886\n",
    "\n",
    "- [NLP 전처리 패딩: pre-padding과 post-padding 비교](https://blog.naver.com/qbxlvnf11/221945962124)\n",
    "\n",
    "\n",
    "pre-padding일 경우에는 앞단에 0이 채워져 마지막 단어로 제로 패딩이 입력으로 들어가는 일 없이 올바른 시퀀스 모델링이 진행되게 됩니다.\n",
    "\n",
    "반면, post-padding일 경우에는 뒷단에 0이 채워져 마지막 단어로 제로 패딩이 입력으로 들어갈 수 있습니다.\n",
    "\n",
    "이는 시퀀스 모델링을 수행하여 뒷단의 입력이 중요한 recurrent model 입장에서 뒤로 갈수록 피처가 희미해지는 long-dependency 현상을 심화시킬 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
